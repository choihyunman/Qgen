import re
import requests
import tempfile
import pdfplumber
import io
from docx import Document
from app.rag.util.clean_text import clean_text


def split_text_by_sentence(text: str, chunk_size: int = 512, overlap: int = 0) -> list[str]:
    print(f"\nğŸª“ [split_text_by_sentence] ì…ë ¥ ê¸¸ì´: {len(text)}")

    sentences = re.split(r'(?<=[.?!])\s+', text)
    print(f"ğŸ”¹ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}")

    chunks = []
    current = ""

    for sentence in sentences:
        if len(current) + len(sentence) <= chunk_size:
            current += sentence + " "
        else:
            chunks.append(current.strip())
            current = sentence + " "

    if current:
        chunks.append(current.strip())

    print(f"ğŸ”¸ 1ì°¨ ì²­í¬ ìˆ˜ (overlap ì „): {len(chunks)}")

    final_chunks = []
    for i in range(len(chunks)):
        start = max(0, i - overlap)
        combined = " ".join(chunks[start:i+1])
        final_chunks.append(combined.strip())

    result = [chunk for chunk in final_chunks if chunk]
    print(f"âœ… ìµœì¢… ì²­í¬ ìˆ˜ (ë¹ˆ ë¬¸ìì—´ ì œê±° í›„): {len(result)}")
    return result


def extract_text_from_bytes(content: bytes, filename: str = "") -> list[str]:
    print(f"\nğŸ“‚ [extract_text_from_bytes] íŒŒì¼ëª…: {filename}, í¬ê¸°: {len(content)} bytes")

    try:
        if filename.endswith(".txt"):
            text = content.decode("utf-8", errors="ignore")
            print(f"ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ: {len(text)}ì")
            return [text]

        elif filename.endswith(".pdf"):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
                temp_file.write(content)
                temp_file_path = temp_file.name

            texts = []
            with pdfplumber.open(temp_file_path) as pdf:
                print(f"ğŸ“„ PDF í˜ì´ì§€ ìˆ˜: {len(pdf.pages)}")
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:
                        print(f"  ğŸ“– Page {i+1}: {len(page_text)}ì | {hash(page_text)}")
                        texts.append(page_text)
            return texts

        elif filename.endswith(".docx"):
            doc = Document(io.BytesIO(content))
            texts = [p.text for p in doc.paragraphs if p.text.strip()]
            print(f"ğŸ“˜ DOCX ë¬¸ë‹¨ ìˆ˜: {len(texts)}")
            return texts

        else:
            print(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {filename}")
            return []

    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


def extract_chunks_from_urls(s3_urls: list[str]) -> list[str]:
    print(f"\nğŸŒ [extract_chunks_from_urls] ì´ URL ìˆ˜: {len(s3_urls)}")
    chunks = []
    for idx, url in enumerate(s3_urls):
        try:
            print(f"\nğŸ”— [{idx+1}] ìš”ì²­ URL: {url}")
            res = requests.get(url)
            res.raise_for_status()

            filename = url.split("?")[0].split("/")[-1]
            texts = extract_text_from_bytes(res.content, filename)

            print(f"ğŸ§¹ ì›ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡ ìˆ˜: {len(texts)}")
            for i, t in enumerate(texts):
                print(f"   â”” [{i+1}] {len(t)}ì | í•´ì‹œ: {hash(t)} | ì‹œì‘: {repr(t[:50])}")

            combined = " ".join(texts)
            cleaned = clean_text(combined)

            print(f"ğŸ§¼ ì •ì œ í›„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(cleaned)} | í•´ì‹œ: {hash(cleaned)}")
            split_chunks = split_text_by_sentence(cleaned)

            for i, c in enumerate(split_chunks):
                print(f"   ğŸ“¦ ì²­í¬ {i+1} | ê¸¸ì´: {len(c)} | í•´ì‹œ: {hash(c)} | ì‹œì‘: {repr(c[:50])}")

            chunks.extend(split_chunks)
        except Exception as e:
            print(f"âŒ URL ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
            continue

    print(f"\nğŸ§¾ ì „ì²´ ìµœì¢… ì²­í¬ ìˆ˜ (ì¤‘ë³µ ì œê±° ì „): {len(chunks)}")
    return chunks