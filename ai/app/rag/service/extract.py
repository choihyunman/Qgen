import io
import re
import hashlib
import logging
from pathlib import Path
from typing import List

import requests
import pdfplumber
from docx import Document
from charset_normalizer import from_bytes

from app.rag.util.clean_text import clean_text

# loggerê°€ ì´ íŒŒì¼ì—ì„œ í•„ìš”í•˜ë¯€ë¡œ ì—†ìœ¼ë©´ ì¶”ê°€
logger = logging.getLogger(__name__)

def split_text_by_sentence(
    text: str,
    chunk_size: int = 512,
    overlap_sentences: int = 1
) -> List[str]:
    # 1) ë¬¸ì¥ ë¶„ë¦¬ (., ?, ! + ê³µë°± ê¸°ì¤€)
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    sentences = [s.strip() for s in sentences if s and s.strip()]

    chunks: List[str] = []
    n = len(sentences)
    i = 0

    while i < n:
        j = i
        current = []
        # 2) iì—ì„œ ì‹œì‘í•´ chunk_sizeë¥¼ ë„˜ê¸° ì „ê¹Œì§€ ë¬¸ì¥ ì¶”ê°€
        while j < n:
            candidate = (" ".join(current + [sentences[j]])).strip()
            if len(candidate) <= chunk_size:
                current.append(sentences[j])
                j += 1
            else:
                break
        # ìµœì†Œ 1ë¬¸ì¥ì€ ë‹´ê¸°ë„ë¡ ë³´ì •
        if not current:
            current = [sentences[j]]
            j += 1

        chunk = " ".join(current).strip()
        if chunk:
            chunks.append(chunk)
        # 3) ë‹¤ìŒ ì‹œì‘ì : jì—ì„œ overlap_sentencesë§Œí¼ ë’¤ë¡œ ë‹¹ê²¨ ê²¹ì¹˜ê¸°
        if j >= n:
            break
        i = max(j - overlap_sentences, i + 1)

    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    return [c for c in chunks if c]


def _safe_decode_txt(content: bytes) -> str:
    # 1) utf-8-sig ìš°ì„ 
    try:
        return content.decode("utf-8-sig")
    except UnicodeDecodeError:
        pass
    # 2) ì¸ì½”ë”© ì¶”ì •
    result = from_bytes(content).best()
    return str(result) if result else content.decode("utf-8", errors="ignore")

def extract_text_from_bytes(content: bytes, filename: str = "") -> list[str]:
    logger.info("[extract_text_from_bytes] íŒŒì¼ëª…=%s, í¬ê¸°=%d bytes", filename, len(content))
    texts: list[str] = []

    try:
        suffix = Path(filename).suffix.lower()

        if suffix == ".txt":
            text = _safe_decode_txt(content)
            logger.info("TXT ì¶”ì¶œ ì™„ë£Œ: %dì, sha256=%s",
                        len(text), hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()[:16])
            return [text]

        elif suffix == ".pdf":
            # ì„ì‹œíŒŒì¼ ì—†ì´ BytesIOë¡œ ë°”ë¡œ ì—°ë‹¤
            with pdfplumber.open(io.BytesIO(content)) as pdf:
                logger.info("PDF í˜ì´ì§€ ìˆ˜: %d", len(pdf.pages))
                for i, page in enumerate(pdf.pages, start=1):
                    try:
                        page_text = page.extract_text() or ""
                        # ê°„ë‹¨ ì •ê·œí™”(ì„ íƒ)
                        page_text = re.sub(r"\s+\n", "\n", page_text).strip()
                        if page_text:
                            digest = hashlib.sha256(page_text.encode("utf-8", errors="ignore")).hexdigest()[:16]
                            logger.debug("  ğŸ“– Page %d: %dì | sha256=%s", i, len(page_text), digest)
                            texts.append(page_text)
                    except Exception as pe:
                        logger.warning("  ğŸ“– Page %d ì¶”ì¶œ ì‹¤íŒ¨: %s", i, pe)
            return texts

        elif suffix == ".docx":
            doc = Document(io.BytesIO(content))
            # ë¬¸ë‹¨ + (ì„ íƒ) í‘œ ì…€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            paragraphs = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]
            # í‘œ í…ìŠ¤íŠ¸ê¹Œì§€ í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ
            # for table in doc.tables:
            #     for row in table.rows:
            #         for cell in row.cells:
            #             t = cell.text.strip()
            #             if t:
            #                 paragraphs.append(t)
            logger.info("DOCX ì¶”ì¶œ ë¸”ë¡ ìˆ˜: %d", len(paragraphs))
            return paragraphs

        else:
            logger.warning("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", filename)
            return []

    except Exception as e:
        logger.exception("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: %s", e)
        return []


def extract_chunks_from_urls(s3_urls: list[str]) -> list[str]:
    print(f"\n [extract_chunks_from_urls] ì´ URL ìˆ˜: {len(s3_urls)}")
    chunks = []
    for idx, url in enumerate(s3_urls):
        try:
            print(f"\nğŸ”— [{idx+1}] ìš”ì²­ URL: {url}")
            res = requests.get(url)
            res.raise_for_status()

            filename = url.split("?")[0].split("/")[-1]
            texts = extract_text_from_bytes(res.content, filename)

            print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡ ìˆ˜: {len(texts)}")
            for i, t in enumerate(texts):
                print(f"   â”” [{i+1}] {len(t)}ì | í•´ì‹œ: {hash(t)} | ì‹œì‘: {repr(t[:50])}")

            combined = " ".join(texts)
            cleaned = clean_text(combined)

            print(f" ì •ì œ í›„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(cleaned)} | í•´ì‹œ: {hash(cleaned)}")
            split_chunks = split_text_by_sentence(cleaned)

            for i, c in enumerate(split_chunks):
                print(f"   ì²­í¬ {i+1} | ê¸¸ì´: {len(c)} | í•´ì‹œ: {hash(c)} | ì‹œì‘: {repr(c[:50])}")

            chunks.extend(split_chunks)
        except Exception as e:
            print(f" URL ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
            continue

    print(f"\n ì „ì²´ ìµœì¢… ì²­í¬ ìˆ˜ (ì¤‘ë³µ ì œê±° ì „): {len(chunks)}")
    return chunks