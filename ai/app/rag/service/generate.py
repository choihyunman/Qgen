import json
import logging
import asyncio
import os
from dotenv import load_dotenv
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor
from app.rag.prompt.prompt import load_choice_prompt, load_oxshort_prompt

load_dotenv()
logger = logging.getLogger(__name__)
executor = ThreadPoolExecutor()
client = OpenAI()

MAX_RETRY = 3
MAX_BATCH_SIZE = 5

def _split_batches(total: int, max_size: int) -> list[int]:
    full, remain = divmod(total, max_size)
    return [max_size] * full + ([remain] if remain else [])

def _split_chunks(chunks: list[str], n: int) -> list[list[str]]:
    if n == 0: return []
    avg = len(chunks) / n
    return [chunks[round(i * avg): round((i + 1) * avg)] for i in range(n)]

def _build_user_message(context: str, q_type: str) -> str:
    base_msg = "ë°˜ë“œì‹œ JSON ë¬¸ìì—´ë¡œë§Œ ì‘ë‹µí•´ì•¼ ë¼. JSON ì™¸ì˜ ì£¼ì„, ì„¤ëª…, ìì—°ì–´ ë¬¸ì¥ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆë¼."
    if q_type == "choice":
        return f"{base_msg} ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ëŠ” '--- ë¬¸ì œ êµ¬ë¶„ ---' ìœ¼ë¡œ ë‚˜ë‰˜ë©° ê° êµ¬ê°„ì€ ë…ë¦½ ë¬¸ì œì•¼:\n\n{context}"
    elif q_type == "oxshort":
        return f"{base_msg} ë‹¤ìŒ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì¤˜:\n\n{context}"
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì œ ìœ í˜•: {q_type}")

def _call_openai(prompt: str, context: str, q_type: str) -> str:
    logger.info(f"\nğŸ§  [GPT ìš”ì²­ - {q_type.upper()}]\n{context[:500]}")
    user_message = _build_user_message(context, q_type)
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": user_message}
        ],
        temperature=1
    )
    raw = response.choices[0].message.content
    logger.info(f"\nğŸ“¦ [GPT ì‘ë‹µ - {q_type.upper()}]\n{raw[:500]}")
    return raw

async def _call_gpt_with_retry(prompt: str, context: str, q_type: str) -> list[dict]:
    for attempt in range(1, MAX_RETRY + 1):
        raw = ""  # ì´ˆê¸°í™”
        try:
            loop = asyncio.get_running_loop()
            raw = await loop.run_in_executor(executor, _call_openai, prompt, context, q_type)

            if not raw or not raw.strip():
                raise ValueError("GPT ì‘ë‹µì´ ë¹„ì–´ ìˆìŒ")

            parsed = json.loads(raw)

            if not isinstance(parsed, list):
                raise ValueError("GPT ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹˜")

            return parsed

        except Exception as e:
            logger.warning(
                f"ğŸ” [GPT ì¬ì‹œë„ {attempt}/{MAX_RETRY} - {q_type}] ì‹¤íŒ¨: {e}"
            )
            if attempt == MAX_RETRY:
                logger.error(
                    f"âŒ GPT ì¬ì‹œë„ ì´ˆê³¼ - {q_type}\n"
                    f"ì—ëŸ¬: {e}\n"
                    f"ì›ë¬¸ ì‘ë‹µ:\n{raw[:1000] if raw else '[ë¹ˆ ë¬¸ìì—´]'}"
                )
                raise

async def generate_problem(choice_chunks: list[str], oxshort_chunks: list[str], choice: int, ox: int, short: int):
    tasks = []

    # âœ… ê°ê´€ì‹ ë¬¸ì œ ë¶„í•  ì²˜ë¦¬
    if choice > 0:
        batches = _split_batches(choice, MAX_BATCH_SIZE)
        chunk_batches = _split_chunks(choice_chunks, len(batches))
        for count, context_chunk in zip(batches, chunk_batches):
            if isinstance(context_chunk, str):
                context_chunk = [context_chunk]

            prompt = load_choice_prompt(count)
            context = "\n--- ë¬¸ì œ êµ¬ë¶„ ---\n".join(context_chunk)
            logger.info(f"ğŸ§ª [ì²­í¬ ìˆ˜: {len(context_chunk)}] ìƒì„±ë  ë¬¸ì œ ìˆ˜: {count}")
            tasks.append(_call_gpt_with_retry(prompt, context, "choice"))


    # âœ… OX + ì£¼ê´€ì‹ ë¬¸ì œ ë¶„í•  ì²˜ë¦¬
    total_oxshort = ox + short
    if total_oxshort > 0:
        batches = _split_batches(total_oxshort, MAX_BATCH_SIZE)
        chunk_batches = _split_chunks(oxshort_chunks, len(batches))

        for count, context_chunk in zip(batches, chunk_batches):
            # âœ… OX, ì£¼ê´€ì‹ ë¹„ìœ¨ ë¶„ë°°
            ox_count = min(count, ox)
            short_count = count - ox_count
            ox -= ox_count
            short -= short_count

            # skip ë¶ˆí•„ìš”í•œ ìš”ì²­
            if ox_count == 0 and short_count == 0:
                continue

            prompt = load_oxshort_prompt(ox_count, short_count)
            context = "\n".join(context_chunk)
            tasks.append(_call_gpt_with_retry(prompt, context, "oxshort"))

    # âœ… GPT í˜¸ì¶œ ì‹¤í–‰
    all_results = await asyncio.gather(*tasks)
    return [item for batch in all_results for item in batch]
