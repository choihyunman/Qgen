import json
import logging
import asyncio
from dotenv import load_dotenv
import os
from openai import OpenAI
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
from app.rag.prompt.prompt import load_choice_prompt, load_oxshort_prompt
import math


load_dotenv()
logger = logging.getLogger(__name__)
executor = ThreadPoolExecutor()
client = OpenAI()

MAX_RETRY = 3

def call_openai(client: OpenAI, prompt: str, context: str, q_type: str) -> str:
    logger.info(f"\nğŸ§  [GPT ìš”ì²­ ì»¨í…ìŠ¤íŠ¸ - {q_type.upper()}]\n" + context[:1000])

    if q_type == "choice":
        user_message = (
            f"ë°˜ë“œì‹œ JSON ë¬¸ìì—´ë¡œë§Œ ì‘ë‹µí•´ì•¼ ë¼. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ê°œì˜ ë¬¸ì œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ìë£ŒëŠ” '--- ë¬¸ì œ êµ¬ë¶„ ---' ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆì–´. ê° êµ¬ë¶„ì ë‹¨ìœ„ë¥¼ ë…ë¦½ëœ ë¬¸ì œë¡œ ê°„ì£¼í•´. í•´ë‹¹ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ë´:\n\n{context}"
        )
    elif q_type == "oxshort":
        user_message = (
            f"ë°˜ë“œì‹œ JSON ë¬¸ìì—´ë¡œë§Œ ì‘ë‹µí•´ì•¼ ë¼. ë‹¤ìŒ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì¤˜:\n\n{context}"
        )
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì œ ìœ í˜•: {q_type}")

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": user_message}
        ],
        temperature=1
    )

    raw = response.choices[0].message.content
    logger.info(f"\nğŸ“¦ [GPT ì‘ë‹µ ê²°ê³¼ - {q_type.upper()}]\n" + raw[:1000])
    return raw

async def _run_gpt(prompt: str, context: str, q_type: str) -> str:
    for attempt in range(1, MAX_RETRY + 1):
        try:
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(
                executor,
                call_openai,
                client,
                prompt,
                context,
                q_type
            )
        except Exception as e:
            logger.warning(f"[GPT ì‹œë„ {attempt} - {q_type}] ì‹¤íŒ¨: {e}")
            if attempt == MAX_RETRY:
                raise RuntimeError(f"{q_type.upper()} GPT í˜¸ì¶œ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼")
            
def _split_batches(total: int, max_batch_size: int) -> list[int]:
    """
    ì´ ë¬¸ì œ ìˆ˜ë¥¼ ìµœëŒ€ max_batch_size í¬ê¸°ë¡œ ë‚˜ëˆˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    ì˜ˆ: total=25 â†’ [10, 10, 5]
    """
    full_batches = total // max_batch_size
    remainder = total % max_batch_size
    return [max_batch_size] * full_batches + ([remainder] if remainder else [])

def _split_chunks(chunks: list[str], num_parts: int) -> list[list[str]]:
    """
    chunksë¥¼ num_parts ê°œìˆ˜ë¡œ ê· ë“±í•˜ê²Œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    if num_parts == 0:
        return []
    avg = len(chunks) / num_parts
    return [chunks[round(i * avg): round((i + 1) * avg)] for i in range(num_parts)]

async def generate_problem(choice_chunks: list[str], oxshort_chunks: list[str], choice: int, ox: int, short: int):
    tasks = []

    # ê°ê´€ì‹ ë¬¸ì œ ìš”ì²­ ë¶„í• 
    if choice > 0:
        choice_batches = _split_batches(choice, max_batch_size=10)
        chunk_batches = _split_chunks(choice_chunks, len(choice_batches))

        for count, context_chunk in zip(choice_batches, chunk_batches):
            prompt = load_choice_prompt(count)
            context = "\n--- ë¬¸ì œ êµ¬ë¶„ ---\n".join(context_chunk)
            tasks.append(_run_gpt(prompt, context, "choice"))

    #OX + ì£¼ê´€ì‹ ë¬¸ì œ ìš”ì²­ ë¶„í•  ---
    total_oxshort = ox + short
    if total_oxshort > 0:
        oxshort_batches = _split_batches(total_oxshort, max_batch_size=10)
        chunk_batches = _split_chunks(oxshort_chunks, len(oxshort_batches))

        # ì²« ë²ˆì§¸ ìš”ì²­ì€ OX ìœ„ì£¼, ê·¸ ë‹¤ìŒì€ SHORT ìœ„ì£¼ë¡œ ë¶„ë°°
        for i, (count, context_chunk) in enumerate(zip(oxshort_batches, chunk_batches)):
            ox_count = min(count, ox) if ox > 0 else 0
            short_count = count - ox_count
            ox -= ox_count
            short -= short_count

            prompt = load_oxshort_prompt(ox_count, short_count)
            context = "\n".join(context_chunk)
            tasks.append(_run_gpt(prompt, context, "oxshort"))

    # GPT ìš”ì²­ ì‹¤í–‰
    gpt_outputs = await asyncio.gather(*tasks)

    # ê²°ê³¼ íŒŒì‹±
    results = []
    for raw in gpt_outputs:
        try:
            if not isinstance(raw, str):
                raise ValueError("GPT ì‘ë‹µì´ ë¬¸ìì—´ì´ ì•„ë‹˜")
            parsed = json.loads(raw)
            assert isinstance(parsed, list)
            results.extend(parsed)
        except Exception as e:
            logger.error(f"âš ï¸ GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ë¬¸:\n{raw[:1000]}")
            raise ValueError(f"GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")

    return results
